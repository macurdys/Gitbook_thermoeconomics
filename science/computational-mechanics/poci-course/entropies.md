# Entropies

**Lecture 14** â€” The information theory toolkit.
ğŸ“º [Watch on YouTube](https://youtu.be/Ys2XCfrmDHY)
ğŸ“„ Reading: Elements of Information Theory, Chapters 1 & 2

---

## Information â‰  Energy

This is the starting point. Information and energy are two different accountings of the same physical system. They are related (via S = kB ln 2 Â· H) but they are not the same thing.

Sources of information:
- **Uncontrolled initial conditions** â€” apparent randomness from ignorance
- **Deterministic chaos** â€” actively generated randomness from dynamics
- **Hidden regularity** â€” ignorance of forces or limited modeling capacity

---

## Shannon Entropy

Derived from axioms (Khinchin or Shannon). The unique measure of uncertainty satisfying continuity, maximum at uniform distribution, and additivity for independent systems:

**H(X) = âˆ’Î£ p(x) logâ‚‚ p(x)**

| Property | Statement |
|----------|-----------|
| Positivity | H(X) â‰¥ 0 |
| Predictable | H(X) = 0 iff one event has probability 1 |
| Maximally random | H(X) = logâ‚‚ k iff uniform over k events |

**Interpretation:** The minimum average number of yes/no questions needed to identify the outcome. The optimal compression rate. The degree of surprise.

---

## Joint, Conditional, Mutual

**Joint entropy** â€” uncertainty in X and Y together:
H(X,Y) = âˆ’Î£ p(x,y) logâ‚‚ p(x,y)

**Conditional entropy** â€” uncertainty in X, knowing Y:
H(X|Y) = H(X,Y) âˆ’ H(Y)

**Mutual information** â€” information shared between X and Y:
I(X;Y) = H(X) âˆ’ H(X|Y) = H(X) + H(Y) âˆ’ H(X,Y)

**KL divergence** â€” information cost of approximating P with Q:
D(Pâ€–Q) = Î£ p(x) logâ‚‚[p(x)/q(x)]

**Information distance** â€” a true metric:
d(X,Y) = H(X|Y) + H(Y|X)

---

## The I-Diagram

```
H(Y|X)  |  I(X;Y)  |  H(X|Y)
â†â€”â€” H(Y) â€”â€”â†’        â†â€”â€” H(X) â€”â€”â†’
â†â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” H(X,Y) â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â†’
```

This diagram encodes all relationships between entropy quantities for two variables. Every information measure is an area in this diagram.

---

## Key Inequalities

- **Conditioning reduces entropy:** H(X|Y) â‰¤ H(X)
- **Independence bound:** H(Xâ‚,...,Xâ‚™) â‰¤ Î£ H(Xáµ¢)
- **Data Processing Inequality:** Xâ†’Yâ†’Z â‡’ I(X;Y) â‰¥ I(X;Z)

The Data Processing Inequality is fundamental: **manipulation cannot increase information about the source.** No amount of post-processing can recover information lost in transmission.

---

## Chain Rules

**Entropy:** H(Xâ‚,...,Xâ‚™) = Î£ H(Xáµ¢ | Xáµ¢â‚‹â‚,...,Xâ‚)

**Mutual Information:** I(Xâ‚,...,Xâ‚™; Y) = Î£ I(Xáµ¢; Y | Xáµ¢â‚‹â‚,...,Xâ‚)

These decompose complex joint quantities into sums of conditional quantities â€” essential for analyzing sequential processes like blockchain state transitions.

---

_This is the vocabulary. The grammar comes in [Lectures 15-20](README.md), where these measures are applied to stochastic processes._

